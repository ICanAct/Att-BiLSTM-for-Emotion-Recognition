{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Preprocess.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ao0MykkU0anE"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re, sys, os, csv\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import Counter\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "id": "acud9WqO9yZJ",
        "outputId": "fbe0ff09-eb61-48d9-8ceb-de2c589a2a27"
      },
      "source": [
        "# get dict of word from english corpus \n",
        "def words(text): return re.findall('[a-z]+', text.lower())\n",
        "dictionary = Counter(words(open('wordlist.txt').read()))\n",
        "max_word_length = max(map(len, dictionary))\n",
        "total = float(sum(dictionary.values()))\n",
        "\n",
        "def word_prob(word): return dictionary[word] / total"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-2d05e2d139ed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'[a-z]+'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdictionary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'wordlist.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mmax_word_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtotal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'wordlist.txt'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yiFtywi-1DZp"
      },
      "source": [
        "def viterbi_segment(text):\n",
        "    probs, lasts = [1.0], [0]\n",
        "    for i in range(1, len(text) + 1):\n",
        "        prob_k, k = max((probs[j] * word_prob(text[j:i]), j)\n",
        "                        for j in range(max(0, i - 45), i))\n",
        "        probs.append(prob_k)\n",
        "        lasts.append(k)\n",
        "    words = []\n",
        "    i = len(text)\n",
        "    while 0 < i:\n",
        "        words.append(text[lasts[i]:i])\n",
        "        i = lasts[i]\n",
        "    words.reverse()\n",
        "    return words, probs[-1]\n",
        "\n",
        "def remove_hashtag(text):\n",
        "    text = text.group().split(\":\")[0]\n",
        "    text = text[1:] \n",
        "    try:\n",
        "        test = int(text[0])\n",
        "        text = text[1:]\n",
        "    except:\n",
        "        pass\n",
        "    output = ' '.join(viterbi_segment(text)[0])\n",
        "    #print(output)\n",
        "    return output\n",
        "\n",
        "def clean_tweet( tweet):\n",
        "        tweet = tweet.lower()\n",
        "        tweet = re.sub(\"(#[A-Za-z0-9]+)\", fix_hashtag, tweet)\n",
        "        return ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \" \", tweet).split())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hg7yQeE95W7r"
      },
      "source": [
        "def combine_labels(label):\n",
        "    if label == \"empty\":return 0 # neutral\n",
        "    elif label == \"sadness\":return 2 # sad\n",
        "    elif label == \"enthusiasm\":return 1 # happy\n",
        "    elif label == \"neutral\":return 0 # neutral\n",
        "    elif label == \"worry\":return 2 # sad\n",
        "    elif label == \"surprise\":return 1 # happy\n",
        "    elif label == \"love\":return 1 # happy\n",
        "    elif label == \"fun\":return 1 # happy\n",
        "    elif label == \"hate\":return 3 #hate\n",
        "    elif label == \"happiness\":return 1 # happy\n",
        "    elif label == \"boredom\":return 0 # neutral\n",
        "    elif label == \"relief\":return 1 # happy\n",
        "    elif label == \"anger\":return 4 #anger"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u3iVI0Sp3Lzd"
      },
      "source": [
        "data_train = pd.read_csv('text_emotion.csv', sep=',')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EHLxuFRI3RaD",
        "outputId": "72c1a5cd-6106-41b9-adc4-da666c296611"
      },
      "source": [
        "dataWriter = csv.writer(open('tweet_data.csv', 'w'), delimiter=',',lineterminator=\"\\n\")\n",
        "\n",
        "total = 40000\n",
        "for i in range(40000):\n",
        "    tweet= clean_tweet(data_train.content[i])\n",
        "    dataWriter.writerow([tweet, str(combine_labels(data_train.sentiment[i]))])\n",
        "print('Completed preprocessing!')\n",
        "    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Completed preprocessing!\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}